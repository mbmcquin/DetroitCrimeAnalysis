{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The United States has a quite a few cities notorious for high rates of violent crime. According to [MSN](https://www.msn.com/en-us/news/crime/25-most-dangerous-cities-in-america/ss-AAsxtw1#image=26), Detroit ranks as the most dangerous city in the country as of 2017 with 303 murders in 2016 alone and a staggering 2,047 violent crimes per 100,000 people. In this tutorial, we will step through the entire data science pipeline, while analyzing crime in the city of Detroit, Michigan.\n",
    "\n",
    "#### Quick Reference:\n",
    "1. [Getting Started](#Getting-Started)\n",
    "1. [Data Curation, Parsing, and Management](#Data-Curation,-Parsing,-and-Management)\n",
    "1. [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n",
    "1. [Machine Learning](#Machine-Learning)\n",
    "1. [Hypothesis Testing](#Hypothesis-Testing)\n",
    "1. [Conclusion](#Conclusion)\n",
    "1. [Resources](#Resources)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reproduce the steps in this notebook, you should be running Python 3.6. \n",
    "\n",
    "You will also need the following packages:\n",
    "1. [Pandas](https://pandas.pydata.org/) (```pip install pandas```)\n",
    "1. [NumPy](http://www.numpy.org/) (```pip install numpy```)\n",
    "1. [Matplotlib](https://matplotlib.org/) (```pip install matplotlib```)\n",
    "1. [Folium](http://folium.readthedocs.io/en/latest/index.html) (```pip install folium```)\n",
    "1. [SciPy](https://www.scipy.org/) (```pip install scipy```)\n",
    "1. [Seaborn](https://seaborn.pydata.org/) (```pip install seaborn```)\n",
    "1. [ScikitLearn](http://scikit-learn.org/stable/index.html) (```pip install sklearn```)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "The links above will navigate you to the pages associated with the packages, in case you run into issues.\n",
    "\n",
    "##### Imports:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "import seaborn as sb\n",
    "import matplotlib.pyplot as plt\n",
    "import folium\n",
    "from datetime import datetime\n",
    "from sklearn import cross_validation\n",
    "import sklearn.metrics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Curation, Parsing, and Management"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to download the [dataset](https://data.detroitmi.gov/api/views/invm-th67/rows.csv?accessType=DOWNLOAD) from the City of Detroit website. This dataset includes crimes recorded January 1, 2009 to December 6, 2016. Click [here](https://data.detroitmi.gov/Public-Safety/DPD-All-Crime-Incidents-January-1-2009-December-6-/invm-th67) to be navigated to the documentation for this dataset which includes column names, column data types, descriptions, etc. In this tutorial, I have saved my data set as 'DetroitCrimeData.csv', and we can import it into Pandas by doing the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-4ff30a135f36>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Create a Pandas dataframe to hold the crime data. We will use this throughout this tutorial.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mcrime_df\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'https://data.detroitmi.gov/api/views/invm-th67/rows.csv?accessType=DOWNLOAD'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparse_dates\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Remove the ROWNUM column as it is unnecessary since we have row indexing built into our dataframe.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mcrime_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ROWNUM'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, skipfooter, skip_footer, doublequote, delim_whitespace, as_recarray, compact_ints, use_unsigned, low_memory, buffer_lines, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    644\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    645\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 646\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    647\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    648\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    373\u001b[0m     filepath_or_buffer, _, compression = get_filepath_or_buffer(\n\u001b[1;32m    374\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 375\u001b[0;31m         compression=kwds.get('compression', None))\n\u001b[0m\u001b[1;32m    376\u001b[0m     kwds['compression'] = (inferred_compression if compression == 'infer'\n\u001b[1;32m    377\u001b[0m                            else compression)\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[0;34m(filepath_or_buffer, encoding, compression)\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;31m# cat on the compression to the tuple returned by the function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    246\u001b[0m         to_return = (list(maybe_read_encoded_stream(req, encoding,\n\u001b[0;32m--> 247\u001b[0;31m                                                     compression)) +\n\u001b[0m\u001b[1;32m    248\u001b[0m                      [compression])\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mto_return\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/site-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mmaybe_read_encoded_stream\u001b[0;34m(reader, encoding, compression)\u001b[0m\n\u001b[1;32m    165\u001b[0m             \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 167\u001b[0;31m             \u001b[0mreader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mStringIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    168\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcompression\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'gzip'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    454\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunked\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 456\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_readall_chunked\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    457\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlength\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_readall_chunked\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    564\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mchunk_left\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    565\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 566\u001b[0;31m                 \u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mchunk_left\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    567\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchunk_left\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    568\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/http/client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[0;34m(self, amt)\u001b[0m\n\u001b[1;32m    610\u001b[0m         \u001b[0ms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    611\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mamt\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 612\u001b[0;31m             \u001b[0mchunk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mMAXAMOUNT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    613\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    614\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mb''\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    584\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mrecv_into\u001b[0;34m(self, buffer, nbytes, flags)\u001b[0m\n\u001b[1;32m   1000\u001b[0m                   \u001b[0;34m\"non-zero flags not allowed in calls to recv_into() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1001\u001b[0m                   self.__class__)\n\u001b[0;32m-> 1002\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1003\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1004\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msocket\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    863\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Read on closed or unwrapped SSL socket.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    864\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 865\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    866\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    867\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.6/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m    623\u001b[0m         \"\"\"\n\u001b[1;32m    624\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mbuffer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 625\u001b[0;31m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    626\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    627\u001b[0m             \u001b[0mv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Create a Pandas dataframe to hold the crime data. We will use this throughout this tutorial.\n",
    "crime_df = pd.read_csv('https://data.detroitmi.gov/api/views/invm-th67/rows.csv?accessType=DOWNLOAD', dtype=str, parse_dates=False)\n",
    "\n",
    "# Remove the ROWNUM column as it is unnecessary since we have row indexing built into our dataframe.\n",
    "del crime_df['ROWNUM']\n",
    "\n",
    "crime_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a violent crime?\n",
    "In order to analyze violent crime, we need to first define what it is. Below you can see a unique list of all the different categories of crimes that this data set covers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Print all categories of crimes\n",
    "crimes = sorted(crime_df.CATEGORY.unique())\n",
    "for i in range(0, len(crimes), 3):\n",
    "    three = crimes[i:(min(i + 3, len(crimes)))]\n",
    "    string = \"{:40} {:40} {:40}\".format(three[0] if len(three) >= 1 else \"\", \\\n",
    "                                        three[1] if len(three) >= 2 else \"\", \\\n",
    "                                        three[2] if len(three) == 3 else \"\")\n",
    "    print(string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will define a \"violent crime\" to be any crime of arson, aggravated assult, assult, homicide, kidnapping, negligent homicide (manslaughter), robbery, or any sex offense. The code below will give us a new data frame that only deals with these offenses. Note that kidnapping is listed twice, as it is incorrectly spelled 'kidnaping' in some places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We are interested in violent crimes, so we first define a list of them:\n",
    "VIOLENT_CRIMES = ['ARSON', 'AGGRAVATED ASSAULT', 'ASSAULT', 'HOMICIDE', 'KIDNAPING', 'KIDNAPPING', 'NEGLIGENT HOMICIDE', \\\n",
    "                  'ROBBERY', 'SEX OFFENSES']\n",
    "\n",
    "# Now, we create a new dataframe with only the above crimes\n",
    "crime_df = crime_df.loc[crime_df['CATEGORY'].isin(VIOLENT_CRIMES)]\n",
    "\n",
    "crime_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Some tidying\n",
    "\n",
    "For the purposes of this tutorial, we only care about the month and year that a crime occurred. We will not worry about the time or day of the month, so we will reformat the INCIDENTDATE column. Also, we need to extract the latitude/longitude from the LOCATION column and transform them into something usable for later. See below!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Transform INCIDENTDATE column into just MM/YYYY\n",
    "crime_df['INCIDENTDATE'] = crime_df['INCIDENTDATE'].map(lambda x: datetime.strptime(str(x).split(' ')[0], '%m/%d/%Y'))\n",
    "\n",
    "# Creates a tuple of (lat,long) from the string current in LOCATION\n",
    "def lat_long_tuple(x):\n",
    "    if '\\n' not in x: return \"Unknown\"\n",
    "    string = str(x).split('\\n')[1]\n",
    "    split = string.split(',')\n",
    "    return (float(split[0][1:]), float(split[1][1:-1]))\n",
    "\n",
    "# Use the function above to transformn the LOCATION column into (lat,long), and remove old LOCATION column\n",
    "crime_df['LAT/LONG'] = crime_df['LOCATION'].map(lat_long_tuple)\n",
    "del crime_df['LOCATION']\n",
    "\n",
    "crime_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    "\n",
    "Now we will use visualization and other statistical techniques to analyze our dataset, find interesting patterns and trends, and form some testable hypothesis. First, let's start with a simple scatterplot showing the number of violent crimes for each day from 01/01/2009 to 12/06/2016."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group the data by date \n",
    "groupby_date = crime_df.groupby('INCIDENTDATE').size().reset_index(name=\"COUNT\")\n",
    "groupby_date.head(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the scatterplot\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "plt.plot_date(groupby_date[\"INCIDENTDATE\"], groupby_date[\"COUNT\"], color=\"#4542f4\")\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Number of Violent Crimes')\n",
    "plt.title('Number of Violent Crimes by Date')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance, one may notice that there appears to be a downwards trend in violent crime over the years 2009 - 2016 in Detroit. Let's try to visually confirm these results by fitting a linear regression with Seaborn!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot with linear regression\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "g = sb.regplot(groupby_date[\"INCIDENTDATE\"].apply(lambda x: x.timestamp()), groupby_date[\"COUNT\"],  \\\n",
    "                 line_kws={'color':'red'}, color=\"#4256f4\", ax=ax)\n",
    "plt.xlabel(\"Date (Represented as seconds since epoch)\")\n",
    "plt.ylabel(\"Number of Violent Crimes\")\n",
    "plt.title('Number of Violent Crimes by Date w/ Regression')\n",
    "sb.plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clearly the linear regression shows a downwards trend in violent crimes from year 2009 to 2016. Upon further inspection, it seems as though each year has roughly the same distrubution. We will later attempt to use machine learning to predict how many violent crimes will occurr on any given day throughout the year!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crime by location in Detroit\n",
    "\n",
    "For fun, let's take a look at a map of the different locations where this violent crime is happening. Hopefully we might get some insight as to which locations in Detroit are the safest. Safest relative to the rest of Detroit that is. After all, we are\n",
    "talking about the most dangerous city in America!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create map, centered on Detroit\n",
    "crime_map = folium.Map(location=[42.337431, -83.048331], zoom_start = 11)\n",
    "\n",
    "# Create a dictionary for referencing different colors for different crimes\n",
    "color_lookup = {'ARSON': 'yellow', 'AGGRAVATED ASSAULT': 'blue', 'ASSAULT': 'orange', 'HOMICIDE': 'red', \\\n",
    "                'KIDNAPING': 'green', 'KIDNAPPING': 'green', 'NEGLIGENT HOMICIDE': 'purple', \\\n",
    "                  'ROBBERY': 'black', 'SEX OFFENSES': '#00FFFF'}\n",
    "\n",
    "# Add a random sample of 1000 data points to the map\n",
    "for index, row in crime_df.sample(1000).iterrows():\n",
    "    lat = row['LAT/LONG'][0]\n",
    "    long = row['LAT/LONG'][1]\n",
    "    category = row['CATEGORY']\n",
    "    folium.CircleMarker([lat, long], color=color_lookup[category],fill=True, fill_color=color_lookup[category], \\\n",
    "                        radius = 2 if row['CATEGORY'] != 'HOMICIDE' else 5, popup=category).add_to(crime_map)\n",
    "\n",
    "crime_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the random sample of violent crimes, it appears as though the Hamtramck/Highland Park regions of Detroit have the lowest rates of such crime."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Machine Learning\n",
    "\n",
    "The hypothesis we are going to test in this section is that we may be able to predict the number of crimes that will happen on any given day in Detroit. To do this, we will use a tool called [Polyfit](https://docs.scipy.org/doc/numpy-1.13.0/reference/generated/numpy.polyfit.html) which resides in the NumPy package. We give Polyfit a polynomial degree, say 8, and a random sample (a training set) from our data, and then we should be able to query for a date and get a prediction of what the crime will be on that day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Just a little more tidying\n",
    "\n",
    "Because we have the years 2009 - 2016 in our data, we need to account for leap years when getting the day number. Years 2012 and 2016 both have the day Feb. 29th, which will push every date in those years back by a day. For instance 12/16/2009 is day 350 of the year, while 12/16/2012 is day 351 of the year. To account for this, we will set the year (since we are not worried about it in this part of the tutorial) to 2012 for every date, that way each year has the same amount of days (366)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace all years with 2012 so each year has an even 366 days\n",
    "group_copy = groupby_date.copy()\n",
    "group_copy['INCIDENTDATE'] = groupby_date['INCIDENTDATE'].apply(lambda x: x.replace(year=2016))\n",
    "\n",
    "# Transform each date into the day of the year\n",
    "group_copy['INCIDENTDATE'] = group_copy['INCIDENTDATE'].apply(lambda x: x.timetuple().tm_yday)\n",
    "\n",
    "group_copy.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Polyfit example\n",
    "Now that we have the data we want, let's do an example that will give a general overview of what is going on. Below, we will take a random sample of 200 days from our dataset and use that to fit a Polyfit polynomial. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create random sample of 200 days from our 2009 - 2016 period, and fit polynomial (degree 8)\n",
    "crime_sample = group_copy.sample(200)\n",
    "fit = np.polyfit(crime_sample['INCIDENTDATE'], crime_sample['COUNT'], 8)\n",
    "poly = np.poly1d(fit)\n",
    "x = np.linspace(1, 366, 100)\n",
    "y = poly(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the data along with our polyfit polynomial\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "plt.plot(x, y)\n",
    "plt.plot(crime_sample['INCIDENTDATE'], crime_sample['COUNT'], '.', color = \"orange\")\n",
    "plt.xlabel('Day Number of Year')\n",
    "plt.ylabel('Number of Violent Crimes')\n",
    "plt.title('Example: Number of Violent Crimes vs Day of the Year')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### Create training and testing data sets\n",
    "In order to properly use Polyfit, we need to break up our data into a training and testing set. We will give Polyfit our training data set and it will learn how to predict number of crimes based on that data. Then, we will test the newly trained Polyfit on our testing set, and record the error compared to the actual number of crimes for that day. We want to ensure that our data size is large, but we also want to ensure that we have enough test data to accurately measure the error of the fit. We will allow our training set to use 3/4 of the entire dataset, while leaving our test set with the remaining 1/4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We take a random sample of the entire set - this ensures random order within the set\n",
    "random_sample = group_copy.sample(frac=1)\n",
    "\n",
    "# Get our training and testing sets\n",
    "split = int(len(random_sample) * (3/4))\n",
    "train = random_sample[0:split]\n",
    "test = random_sample[split::]\n",
    "\n",
    "print(\"Training set now has: {} Samples\".format(len(train)))\n",
    "print(\"Testing  set now has: {}  Samples\".format(len(test)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Choosing the best polynomial degree with Cross Validation\n",
    "Generally, we would now start our [cross-validation](http://scikit-learn.org/stable/modules/cross_validation.html). This would ensure we have the best possible polynomial degree so that our training is as accurate as possible. We do this by iterating through a few possible choices of the polynomial degree and computing the error given by that degree. Once we are done, we would choose to use the degree which minimized the error in our tests. We will leave that out of this tutorial and just assume a polynomial degree of 8. We are ready to fully train Polyfit with our data!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train data with out random trainings set\n",
    "fit = np.polyfit(train[\"INCIDENTDATE\"], train[\"COUNT\"], 8)\n",
    "poly = np.poly1d(fit)\n",
    "x = np.linspace(1, 366, 100)\n",
    "y = poly(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Plot the test data along with the Polyfit prediction!\n",
    "fig, ax = plt.subplots(figsize=(16,8))\n",
    "plt.plot(x, y)\n",
    "plt.plot(test['INCIDENTDATE'], test['COUNT'], '.', color = \"purple\")\n",
    "plt.xlabel('Day Number of Year')\n",
    "plt.ylabel('Number of Violent Crimes')\n",
    "plt.title('Number of Violent Crimes vs Day of the Year w/ Prediction')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hypothesis Testing\n",
    "Now comes time to test how accurate our hypothesis was. We wanted to show that there was a correlation between the day of the year and the number of violent crimes that occurred. For our testing, let's use the R-squared score. To understand how R-squared values work, feel free to check out [this](http://statisticsbyjim.com/regression/interpret-r-squared-regression/) blog post. We are looking for an R-squared value of close to 1 to represent a positive correlation between day of the year and number of violent crimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Compute R-squared using Scilearn\n",
    "r_squared = sklearn.metrics.r2_score(test['COUNT'], poly(test['INCIDENTDATE']))\n",
    "print(r_squared)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this R-squared value is low, it might be enough to support our hypothesis. The value would directly interpret that we do not explain ~70% of the error in our prediction. However, this does not mean that we do not have a positive correlation between day of the year and number of violent crimes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "Detroit is a dangerous city, and local governments are trying everything they can to keep the city safe. Being able to analyze the crime and make decisions based on it can be critical to increasing safety. In this tutorial, we showed how crime actually seems to be decreasing year-over-year. We have visualized the locations of these violent crimes on a map, which could help governments to ensure that areas with dense violent crime rates have more police presence. And we have also seen that violent crime my be correlated with the day of the year. This could potentailly help local police schedule more officers on some days, and less officers on others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "1. MSN: https://www.msn.com/en-us/news/crime/25-most-dangerous-cities-in-america/ss-AAsxtw1#image=26\n",
    "1. R-Squared Blog: http://statisticsbyjim.com/regression/interpret-r-squared-regression/\n",
    "1. Cross Validation: http://scikit-learn.org/stable/modules/cross_validation.html\n",
    "1. Pandas: https://pandas.pydata.org/\n",
    "1. NumPy: http://www.numpy.org/\n",
    "1. Matplotlib: https://matplotlib.org/\n",
    "1. Folium: http://folium.readthedocs.io/en/latest/index.html\n",
    "1. SciPy: https://www.scipy.org/\n",
    "1. Seaborn: https://seaborn.pydata.org/\n",
    "1. ScikitLearn: http://scikit-learn.org/stable/index.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
